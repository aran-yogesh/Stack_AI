# Stack AI Vector Database

A high-performance vector database implementation built with Python, FastAPI, and Pydantic for the Stack AI take-home assignment.

## ğŸš€ Features

### Core Features
- **CRUD Operations**: Complete Create, Read, Update, Delete for Libraries, Documents, and Chunks
- **Vector Embeddings**: Integration with Cohere API for generating 1024-dimensional embeddings
- **Dual Indexing**: Both Flat (brute force) and IVF-Flat (clustered) search algorithms
- **k-NN Search**: Fast similarity search with configurable result count
- **Metadata Filtering**: Filter search results by metadata attributes
- **Cascade Deletion**: Automatic cleanup of related entities
- **CSV Export**: Export data for analysis and visualization
- **Thread-Safe**: Concurrent read/write operations with proper locking
- **RESTful API**: Clean HTTP endpoints with proper status codes

### Technical Highlights
- **Domain-Driven Design**: Clean separation of concerns with API, Service, Repository, and Model layers
- **SOLID Principles**: Single responsibility, dependency injection, and interface segregation
- **Type Safety**: Full static typing with Pydantic models
- **Error Handling**: Comprehensive exception handling with proper HTTP status codes
- **Testing**: Unit tests and integration tests for critical functionality
- **Docker Support**: Containerized deployment ready

## ğŸ—ï¸ Architecture

```
app/
â”œâ”€â”€ api/                 # FastAPI route handlers
â”‚   â”œâ”€â”€ chunks.py       # Chunk CRUD endpoints
â”‚   â”œâ”€â”€ csv_export.py   # CSV export endpoints
â”‚   â”œâ”€â”€ documents.py    # Document CRUD endpoints
â”‚   â”œâ”€â”€ libraries.py    # Library CRUD endpoints
â”‚   â””â”€â”€ search.py       # Search and indexing endpoints
â”œâ”€â”€ config.py           # Application configuration
â”œâ”€â”€ indexing/           # Vector indexing algorithms
â”‚   â”œâ”€â”€ base_index.py   # Abstract base class
â”‚   â”œâ”€â”€ flat_index.py   # Brute force search
â”‚   â””â”€â”€ ivf_index.py    # Clustered search
â”œâ”€â”€ models/             # Pydantic data models
â”‚   â””â”€â”€ __init__.py     # Model definitions
â”œâ”€â”€ repositories/       # Data access layer
â”‚   â”œâ”€â”€ base_repository.py  # Abstract repository
â”‚   â””â”€â”€ shared.py       # Shared repository instances
â”œâ”€â”€ services/           # Business logic layer
â”‚   â”œâ”€â”€ chunk_service.py    # Chunk business logic
â”‚   â”œâ”€â”€ document_service.py # Document business logic
â”‚   â”œâ”€â”€ embedding_service.py # Cohere integration
â”‚   â”œâ”€â”€ library_service.py  # Library business logic
â”‚   â”œâ”€â”€ search_service.py   # Search orchestration
â”‚   â””â”€â”€ service_manager.py  # Dependency injection
â””â”€â”€ utils/              # Utility functions
    â””â”€â”€ concurrency.py  # Thread-safe collections
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.8+
- Cohere API key

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd stack-ai-vector-db
   ```

2. **Create virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Set environment variables**
   ```bash
   # REQUIRED: Set your Cohere API key
   export COHERE_API_KEY="your_cohere_api_key_here"
   
   # Or create a .env file
   cp env.example .env
   # Then edit .env with your actual API key
   ```

5. **Run the server**
   ```bash
   python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
   ```

6. **Test the API**
   ```bash
   python test_complete_system.py
   ```

### Using Docker

```bash
# Build the image
docker build -t stack-ai-vector-db .

# Run the container
docker run -p 8000:8000 -e COHERE_API_KEY="your_key" stack-ai-vector-db
```

## ğŸ“š API Documentation

Once the server is running, visit:
- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

### Core Endpoints

#### Libraries
- `POST /libraries/` - Create a library
- `GET /libraries/` - List all libraries
- `GET /libraries/{id}` - Get library by ID
- `PUT /libraries/{id}` - Update library
- `DELETE /libraries/{id}` - Delete library

#### Documents
- `POST /libraries/{id}/documents/` - Create document
- `GET /libraries/{id}/documents/` - List documents
- `GET /libraries/{id}/documents/{doc_id}` - Get document
- `PUT /libraries/{id}/documents/{doc_id}` - Update document
- `DELETE /libraries/{id}/documents/{doc_id}` - Delete document

#### Chunks
- `POST /libraries/{id}/documents/{doc_id}/chunks/` - Create chunk
- `GET /libraries/{id}/documents/{doc_id}/chunks/` - List chunks
- `GET /libraries/{id}/documents/{doc_id}/chunks/{chunk_id}` - Get chunk
- `PUT /libraries/{id}/documents/{doc_id}/chunks/{chunk_id}` - Update chunk
- `DELETE /libraries/{id}/documents/{doc_id}/chunks/{chunk_id}` - Delete chunk

#### Search
- `POST /libraries/{id}/index` - Build search index
- `POST /libraries/{id}/search` - Search chunks

#### Export
- `GET /csv/export` - Export all data to CSV

## ğŸ” Usage Examples

### Create a Library
```bash
curl -X POST "http://localhost:8000/libraries/" \
  -H "Content-Type: application/json" \
  -d '{"name": "My Library", "description": "A sample library"}'
```

### Add a Document
```bash
curl -X POST "http://localhost:8000/libraries/{library_id}/documents/" \
  -H "Content-Type: application/json" \
  -d '{"title": "Sample Document", "content": "This is sample content about machine learning."}'
```

### Create a Chunk
```bash
curl -X POST "http://localhost:8000/libraries/{library_id}/documents/{document_id}/chunks/" \
  -H "Content-Type: application/json" \
  -d '{"text": "Machine learning is a subset of AI.", "metadata": {"topic": "ai"}}'
```

### Build Search Index
```bash
curl -X POST "http://localhost:8000/libraries/{library_id}/index"
```

### Search Chunks
```bash
curl -X POST "http://localhost:8000/libraries/{library_id}/search" \
  -H "Content-Type: application/json" \
  -d '{"query_text": "machine learning", "k": 5}'
```

## ğŸ§ª Testing

### Quick Setup and Testing
```bash
# Easy setup and run all examples
python examples/setup_examples.py --all

# Or run specific test types
python examples/run_tests.py --test-type crud
python examples/run_tests.py --test-type tests
python examples/run_tests.py --test-type benchmark
```

### Comprehensive Examples
The `examples/` directory contains complete examples and testing utilities:

- **`crud_examples.py`**: Complete CRUD operations demonstration
- **`testing_examples.py`**: Comprehensive test suite with performance benchmarks
- **`api_usage_examples.md`**: Detailed API usage with cURL and Python examples
- **`run_tests.py`**: Test runner for easy execution
- **`setup_examples.py`**: Setup script for environment preparation

### Run All Tests
```bash
python test_complete_system.py
```

### Test Individual Components
```bash
# Test API health
curl http://localhost:8000/health

# Test core functionality
python -c "
import asyncio
import httpx

async def test():
    async with httpx.AsyncClient() as client:
        response = await client.get('http://localhost:8000/')
        print(response.json())

asyncio.run(test())
"
```

### Example Test Results
```
ğŸ§ª Running Comprehensive Test Suite
============================================================
ğŸ¥ Testing API Health...
âœ… API health check passed

ğŸ›ï¸  Testing Library CRUD...
âœ… Library CRUD tests passed

ğŸ“„ Testing Document CRUD...
âœ… Document CRUD tests passed

ğŸ§© Testing Chunk CRUD...
âœ… Chunk CRUD tests passed

ğŸ” Testing Search Operations...
âœ… Search operation tests passed

âš¡ Testing Concurrent Operations...
âœ… Created 10 chunks concurrently in 0.15 seconds
âœ… Performed 5 concurrent searches in 0.08 seconds

ğŸ“Š Testing Large Dataset...
âœ… Created 100 chunks in 2.34 seconds
âœ… Built index in 0.45 seconds
âœ… Average search time: 0.023 seconds

ğŸ‰ All tests passed successfully!
```

## âš™ï¸ Configuration

The application can be configured via environment variables:

- `COHERE_API_KEY` - Required. Your Cohere API key
- `COHERE_MODEL` - Optional. Default: "embed-english-v3.0"
- `EMBEDDING_DIMENSION` - Optional. Default: 1024
- `MAX_CHUNK_SIZE` - Optional. Default: 1000
- `DEFAULT_K` - Optional. Default: 10

## ğŸ”§ Development

### Project Structure
The project follows Domain-Driven Design principles with clear separation of concerns:

- **API Layer**: FastAPI route handlers
- **Service Layer**: Business logic and orchestration
- **Repository Layer**: Data access abstraction
- **Model Layer**: Pydantic data models

### Key Design Patterns
- **Dependency Injection**: Services are injected via ServiceManager
- **Repository Pattern**: Abstract data access layer
- **Strategy Pattern**: Pluggable indexing algorithms
- **Factory Pattern**: Service instantiation

### Thread Safety
All data structures use thread-safe collections with proper locking to ensure concurrent access safety.

## ğŸ“Š Performance

### Indexing Performance
- **Flat Index**: O(n) build time, O(n) search time
- **IVF Index**: O(n log n) build time, O(k + n/k) search time

### Memory Usage
- **Flat Index**: ~0.004MB per 1000 chunks
- **IVF Index**: ~0.4MB per 1000 chunks

### Search Performance
- Typical search time: <300ms for 1000+ chunks
- Supports concurrent searches
- Configurable result count (k)

## ğŸ› Troubleshooting

### Common Issues

1. **Cohere API Key Missing**
   ```bash
   export COHERE_API_KEY="your_key_here"
   ```

2. **Port Already in Use**
   ```bash
   python -m uvicorn app.main:app --port 8001
   ```

3. **Import Errors**
   ```bash
   pip install -r requirements.txt
   ```

### Debug Mode
```bash
python -m uvicorn app.main:app --reload --log-level debug
```

## ğŸ“„ License

This project is part of the Stack AI take-home assignment.

## ğŸ¤ Contributing

This is a take-home assignment submission. For questions or issues, please refer to the assignment requirements.